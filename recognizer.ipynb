{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784  # input (28x28 pixels)\n",
    "n_hidden1 = 512  # 1st hidden\n",
    "n_hidden2 = 256  # 2nd hidden\n",
    "n_hidden3 = 128  # 3rd hidde\n",
    "n_output = 10  # output  (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "n_iterations = 1000\n",
    "batch_size = 128\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1': tf.Variable(tf.truncated_normal([n_input, n_hidden1], stddev=0.1)),\n",
    "    'w2': tf.Variable(tf.truncated_normal([n_hidden1, n_hidden2], stddev=0.1)),\n",
    "    'w3': tf.Variable(tf.truncated_normal([n_hidden2, n_hidden3], stddev=0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden3, n_output], stddev=0.1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.constant(0.1, shape=[n_hidden1])),\n",
    "    'b2': tf.Variable(tf.constant(0.1, shape=[n_hidden2])),\n",
    "    'b3': tf.Variable(tf.constant(0.1, shape=[n_hidden3])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-4c6ba2d2ed2c>:4: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "layer_drop = tf.nn.dropout(layer_3, keep_prob)\n",
    "output_layer = tf.matmul(layer_3, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a712c0a48de1>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=Y, logits=output_layer\n",
    "        ))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \t| Loss = 3.2519498 \t| Accuracy = 0.1171875\n",
      "Iteration 100 \t| Loss = 0.57492656 \t| Accuracy = 0.828125\n",
      "Iteration 200 \t| Loss = 0.49849096 \t| Accuracy = 0.875\n",
      "Iteration 300 \t| Loss = 0.32916522 \t| Accuracy = 0.90625\n",
      "Iteration 400 \t| Loss = 0.29691666 \t| Accuracy = 0.9296875\n",
      "Iteration 500 \t| Loss = 0.30811557 \t| Accuracy = 0.9296875\n",
      "Iteration 600 \t| Loss = 0.28397188 \t| Accuracy = 0.8984375\n",
      "Iteration 700 \t| Loss = 0.348062 \t| Accuracy = 0.9296875\n",
      "Iteration 800 \t| Loss = 0.288534 \t| Accuracy = 0.90625\n",
      "Iteration 900 \t| Loss = 0.27676618 \t| Accuracy = 0.953125\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iterations):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    sess.run(train_step, feed_dict={\n",
    "        X: batch_x, Y: batch_y, keep_prob: dropout\n",
    "        })\n",
    "\n",
    "    # loss and acc per minibach\n",
    "    if i % 100 == 0:\n",
    "        minibatch_loss, minibatch_accuracy = sess.run(\n",
    "            [cross_entropy, accuracy],\n",
    "            feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0}\n",
    "            )\n",
    "        print(\n",
    "            \"Iteration \",\n",
    "            str(i),\n",
    "            \"\\t| Loss is \",\n",
    "            str(minibatch_loss),\n",
    "            \"\\t| Accuracy is \",\n",
    "            str(minibatch_accuracy)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on test set: 0.9172\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0})\n",
    "print(\"\\nAccuracy on test set:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def init_model():\n",
    "\n",
    "    ###  Model Creation\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='Valid', activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same', activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='Valid', activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same', activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(519, activation=\"relu\"))  # [[521,0.9962,70],[519,0.9969,51]\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "    annealer = ReduceLROnPlateau(monitor='val_acc', patience=1, verbose=2, factor=0.5, min_lr=0.0000001) #patience=2\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        zoom_range=0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False)\n",
    "\n",
    "    return model, annealer, datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n",
      "{1: 4684, 7: 4401, 3: 4351, 9: 4188, 2: 4177, 6: 4137, 0: 4132, 4: 4072, 8: 4063, 5: 3795}\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "590/590 [==============================] - 180s 305ms/step - loss: 0.3545 - accuracy: 0.8856 - val_loss: 0.0330 - val_accuracy: 0.9684\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/keras/callbacks/callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590/590 [==============================] - 179s 303ms/step - loss: 0.1036 - accuracy: 0.9677 - val_loss: 0.0473 - val_accuracy: 0.9774\n",
      "Epoch 3/30\n",
      "590/590 [==============================] - 180s 305ms/step - loss: 0.0797 - accuracy: 0.9765 - val_loss: 0.1076 - val_accuracy: 0.9831\n",
      "Epoch 4/30\n",
      "590/590 [==============================] - 188s 319ms/step - loss: 0.0659 - accuracy: 0.9804 - val_loss: 0.0138 - val_accuracy: 0.9862\n",
      "Epoch 5/30\n",
      "590/590 [==============================] - 186s 316ms/step - loss: 0.0599 - accuracy: 0.9817 - val_loss: 0.0758 - val_accuracy: 0.9867\n",
      "Epoch 6/30\n",
      "590/590 [==============================] - 187s 317ms/step - loss: 0.0564 - accuracy: 0.9827 - val_loss: 0.1313 - val_accuracy: 0.9883\n",
      "Epoch 7/30\n",
      "590/590 [==============================] - 188s 318ms/step - loss: 0.0485 - accuracy: 0.9852 - val_loss: 0.0060 - val_accuracy: 0.9901\n",
      "Epoch 8/30\n",
      "590/590 [==============================] - 182s 308ms/step - loss: 0.0463 - accuracy: 0.9857 - val_loss: 0.0258 - val_accuracy: 0.9891\n",
      "Epoch 9/30\n",
      "590/590 [==============================] - 193s 327ms/step - loss: 0.0437 - accuracy: 0.9869 - val_loss: 0.0335 - val_accuracy: 0.9881\n",
      "Epoch 10/30\n",
      "590/590 [==============================] - 228s 387ms/step - loss: 0.0409 - accuracy: 0.9877 - val_loss: 0.0293 - val_accuracy: 0.9904\n",
      "Epoch 11/30\n",
      "590/590 [==============================] - 226s 383ms/step - loss: 0.0384 - accuracy: 0.9886 - val_loss: 0.0602 - val_accuracy: 0.9878\n",
      "Epoch 12/30\n",
      "590/590 [==============================] - 215s 364ms/step - loss: 0.0364 - accuracy: 0.9893 - val_loss: 0.0992 - val_accuracy: 0.9867\n",
      "Epoch 13/30\n",
      "590/590 [==============================] - 214s 362ms/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 0.0032 - val_accuracy: 0.9915\n",
      "Epoch 14/30\n",
      "590/590 [==============================] - 214s 363ms/step - loss: 0.0350 - accuracy: 0.9896 - val_loss: 0.0017 - val_accuracy: 0.9912\n",
      "Epoch 15/30\n",
      "590/590 [==============================] - 218s 369ms/step - loss: 0.0344 - accuracy: 0.9899 - val_loss: 0.0114 - val_accuracy: 0.9918\n",
      "Epoch 16/30\n",
      "590/590 [==============================] - 220s 373ms/step - loss: 0.0327 - accuracy: 0.9905 - val_loss: 0.0029 - val_accuracy: 0.9912\n",
      "Epoch 17/30\n",
      "590/590 [==============================] - 229s 388ms/step - loss: 0.0320 - accuracy: 0.9905 - val_loss: 0.0442 - val_accuracy: 0.9910\n",
      "Epoch 18/30\n",
      "590/590 [==============================] - 244s 413ms/step - loss: 0.0352 - accuracy: 0.9897 - val_loss: 4.8610e-04 - val_accuracy: 0.9929\n",
      "Epoch 19/30\n",
      "590/590 [==============================] - 227s 384ms/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.0328 - val_accuracy: 0.9901\n",
      "Epoch 20/30\n",
      "590/590 [==============================] - 230s 390ms/step - loss: 0.0308 - accuracy: 0.9913 - val_loss: 0.0629 - val_accuracy: 0.9918\n",
      "Epoch 21/30\n",
      "590/590 [==============================] - 218s 370ms/step - loss: 0.0287 - accuracy: 0.9916 - val_loss: 0.0027 - val_accuracy: 0.9912\n",
      "Epoch 22/30\n",
      "590/590 [==============================] - 208s 353ms/step - loss: 0.0280 - accuracy: 0.9923 - val_loss: 0.0023 - val_accuracy: 0.9909\n",
      "Epoch 23/30\n",
      "590/590 [==============================] - 214s 362ms/step - loss: 0.0306 - accuracy: 0.9911 - val_loss: 0.0051 - val_accuracy: 0.9925\n",
      "Epoch 24/30\n",
      "590/590 [==============================] - 217s 368ms/step - loss: 0.0293 - accuracy: 0.9919 - val_loss: 9.1152e-04 - val_accuracy: 0.9926\n",
      "Epoch 25/30\n",
      "590/590 [==============================] - 217s 368ms/step - loss: 0.0272 - accuracy: 0.9921 - val_loss: 0.0256 - val_accuracy: 0.9901\n",
      "Epoch 26/30\n",
      "590/590 [==============================] - 237s 401ms/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 0.0328 - val_accuracy: 0.9924\n",
      "Epoch 27/30\n",
      "590/590 [==============================] - 248s 420ms/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.0011 - val_accuracy: 0.9935\n",
      "Epoch 28/30\n",
      "590/590 [==============================] - 214s 363ms/step - loss: 0.0267 - accuracy: 0.9921 - val_loss: 0.0070 - val_accuracy: 0.9937\n",
      "Epoch 29/30\n",
      "590/590 [==============================] - 206s 349ms/step - loss: 0.0267 - accuracy: 0.9928 - val_loss: 7.9435e-04 - val_accuracy: 0.9924\n",
      "Epoch 30/30\n",
      "590/590 [==============================] - 220s 373ms/step - loss: 0.0272 - accuracy: 0.9921 - val_loss: 0.0440 - val_accuracy: 0.9946\n",
      "4200/4200 [==============================] - 6s 1ms/step\n",
      "Test accuracy:  0.9957143068313599\n",
      "Saved model to disk\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4ef851a077f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Get the data\n",
    "train = pd.read_csv(\"./input/train.csv\")\n",
    "print(train.shape)\n",
    "\n",
    "#Prepare data\n",
    "y = train[\"label\"]\n",
    "X = train.drop(\"label\", axis = 1)\n",
    "print(y.value_counts().to_dict())\n",
    "y = to_categorical(y, num_classes = 10)\n",
    "del train\n",
    "\n",
    "X = X / 255.0\n",
    "X = X.values.reshape(-1,28,28,1)\n",
    "\n",
    "# Do train and test splitting + shuffling\n",
    "seed=2\n",
    "train_index, valid_index = ShuffleSplit(n_splits=1,\n",
    "                                        train_size=0.9,\n",
    "                                        test_size=None,\n",
    "                                        random_state=seed).split(X).__next__()\n",
    "x_train = X[train_index]\n",
    "Y_train = y[train_index]\n",
    "x_test = X[valid_index]\n",
    "Y_test = y[valid_index]\n",
    "\n",
    "# Params\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "validation_steps = 10000\n",
    "\n",
    "# initialize Model, Annealer and Datagen\n",
    "model, annealer, datagen = init_model()\n",
    "\n",
    "# Train\n",
    "train_generator = datagen.flow(x_train, Y_train, batch_size=batch_size)\n",
    "test_generator = datagen.flow(x_test, Y_test, batch_size=batch_size)\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=x_train.shape[0]//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=validation_steps//batch_size,\n",
    "                    callbacks=[annealer])\n",
    "\n",
    "score = model.evaluate(x_test, Y_test)\n",
    "print('Test accuracy: ', score[1])\n",
    "\n",
    "# Save model\n",
    "model.save('Digits-1.3.0.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 784)\n",
      "Base model scores:\n",
      "[[411   0   0   0   0   0   0   0   0   0]\n",
      " [  0 483   0   0   0   0   0   1   1   0]\n",
      " [  0   0 402   0   0   0   0   1   0   0]\n",
      " [  0   1   0 412   0   2   0   0   3   0]\n",
      " [  0   0   1   0 458   0   1   0   0   1]\n",
      " [  0   0   0   0   0 371   0   0   1   0]\n",
      " [  0   0   0   0   0   1 411   0   1   0]\n",
      " [  0   0   0   0   0   0   0 446   0   0]\n",
      " [  0   0   0   0   0   0   0   0 381   1]\n",
      " [  0   0   0   0   1   0   0   1   0 407]]\n",
      "Elapsed time: 01:47:29\n"
     ]
    }
   ],
   "source": [
    "## Predict on test\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "print(test.shape)\n",
    "test = test / 255\n",
    "test = test.values.reshape(-1, 28, 28, 1)\n",
    "p = np.argmax(model.predict(test), axis=1)\n",
    "\n",
    "print('Base model scores:')\n",
    "valid_loss, valid_acc = model.evaluate(x_test, Y_test, verbose=0)\n",
    "valid_p = np.argmax(model.predict(x_test), axis=1)\n",
    "target = np.argmax(Y_test, axis=1)\n",
    "cm = confusion_matrix(target, valid_p)\n",
    "print(cm)\n",
    "\n",
    "## Prepare for submission\n",
    "submission = pd.DataFrame(pd.Series(range(1, p.shape[0] + 1), name='ImageId'))\n",
    "submission['Label'] = p\n",
    "filename=\"keras-cnn-{0}.csv\".format(str(int(score[1]*10000)))\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {0}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
